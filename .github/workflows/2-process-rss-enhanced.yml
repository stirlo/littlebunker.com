name: "2. Process Multi-Category RSS Feeds with Smart SEO"

on:
  workflow_run:
    workflows: ["1. Fetch ALL Climate RSS Feeds"]
    types: [completed]
  workflow_dispatch:

jobs:
  process-feeds:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install feedparser pyyaml beautifulsoup4

      - name: Process Multi-Category RSS Feeds with Smart SEO
        run: |
          python3 << 'EOF'
          import feedparser
          import json
          import datetime
          import re
          import os
          import hashlib
          from datetime import datetime, timezone, timedelta
          import glob
          from collections import Counter

          # Smart tagging system
          class SmartTagger:
              def __init__(self):
                  self.tag_mappings = {
                      # Weather events
                      'hurricane': ['hurricane', 'tropical-storms', 'extreme-weather'],
                      'typhoon': ['typhoon', 'tropical-storms', 'extreme-weather'],
                      'cyclone': ['cyclone', 'tropical-storms', 'extreme-weather'],
                      'drought': ['drought', 'water-crisis', 'extreme-weather'],
                      'flood': ['flooding', 'extreme-weather', 'disasters'],
                      'flooding': ['flooding', 'extreme-weather', 'disasters'],
                      'wildfire': ['wildfires', 'extreme-weather', 'forest-fires'],
                      'heatwave': ['heatwave', 'extreme-weather', 'temperature-records'],
                      'heat wave': ['heatwave', 'extreme-weather', 'temperature-records'],

                      # Climate science terms
                      'arctic': ['arctic', 'polar-regions', 'ice-melt'],
                      'antarctic': ['antarctica', 'polar-regions', 'ice-melt'],
                      'glacier': ['glaciers', 'ice-loss', 'climate-indicators'],
                      'permafrost': ['permafrost', 'arctic', 'tipping-points'],
                      'sea level': ['sea-level-rise', 'coastal-impacts', 'ocean-changes'],
                      'ocean acidification': ['ocean-acidification', 'marine-impacts', 'carbon-cycle'],
                      'el ni√±o': ['el-nino', 'weather-patterns', 'pacific'],
                      'la ni√±a': ['la-nina', 'weather-patterns', 'pacific'],

                      # Geographic regions
                      'pacific': ['pacific-region', 'oceania'],
                      'atlantic': ['atlantic-region', 'hurricanes'],
                      'california': ['california', 'usa', 'west-coast'],
                      'australia': ['australia', 'oceania', 'southern-hemisphere'],
                      'amazon': ['amazon', 'rainforest', 'south-america'],
                      'sahel': ['sahel', 'africa', 'desertification'],

                      # Impact areas
                      'agriculture': ['agriculture', 'food-security', 'farming'],
                      'migration': ['climate-migration', 'displacement', 'refugees'],
                      'health': ['public-health', 'climate-health', 'disease'],
                      'economic': ['economic-impacts', 'climate-costs', 'finance'],
                      'insurance': ['insurance', 'climate-risk', 'economic-impacts'],

                      # Energy and solutions
                      'renewable': ['renewable-energy', 'clean-energy', 'solutions'],
                      'solar': ['solar-power', 'renewable-energy', 'clean-tech'],
                      'wind': ['wind-power', 'renewable-energy', 'clean-tech'],
                      'fossil fuel': ['fossil-fuels', 'emissions', 'oil-gas'],
                      'carbon capture': ['carbon-capture', 'ccs', 'mitigation'],

                      # Policy and governance
                      'paris agreement': ['paris-agreement', 'climate-policy', 'international'],
                      'cop28': ['cop28', 'climate-summit', 'negotiations'],
                      'cop29': ['cop29', 'climate-summit', 'negotiations'],
                      'ipcc': ['ipcc', 'climate-science', 'assessments'],
                      'net zero': ['net-zero', 'climate-targets', 'policy'],
                  }

                  # Keywords that indicate urgency/breaking news
                  self.urgency_keywords = [
                      'breaking', 'urgent', 'emergency', 'crisis', 'unprecedented',
                      'record', 'extreme', 'catastrophic', 'devastating'
                  ]

              def extract_tags(self, title, content, category):
                  """Extract smart tags from title and content"""
                  tags = set()
                  full_text = f"{title} {content}".lower()

                  # Add category-based default tags
                  category_defaults = {
                      'climate-science': ['climate-science', 'research'],
                      'extreme-weather': ['extreme-weather', 'weather-events'],
                      'environmental-news': ['environment', 'news'],
                      'social-impact': ['social-impact', 'communities'],
                      'research-papers': ['research', 'studies', 'peer-reviewed'],
                      'media': ['media-coverage', 'climate-communication']
                  }

                  if category in category_defaults:
                      tags.update(category_defaults[category])

                  # Extract mapped tags
                  for keyword, tag_list in self.tag_mappings.items():
                      if keyword in full_text:
                          tags.update(tag_list[:2])  # Limit tags per keyword

                  # Check for urgency
                  for urgency_word in self.urgency_keywords:
                      if urgency_word in full_text:
                          tags.add('urgent')
                          break

                  # Extract year if recent
                  year_match = re.search(r'202[3-9]', full_text)
                  if year_match:
                      tags.add(f'year-{year_match.group()}')

                  # Limit total tags
                  return list(tags)[:10]

              def generate_seo_description(self, title, content, max_length=160):
                  """Generate SEO-friendly description"""
                  # Clean content
                  clean_text = re.sub(r'<[^>]+>', '', content)
                  clean_text = re.sub(r'\s+', ' ', clean_text).strip()

                  # Remove LaTeX and formulas for SEO description
                  clean_text = clean_text.replace('\\', ' ')
                  clean_text = re.sub(r'\{\{.*?\}\}', '', clean_text)

                  # Try to find a good sentence
                  sentences = re.split(r'[.!?]', clean_text)

                  # Look for sentences with key information
                  for sentence in sentences[:3]:
                      if len(sentence) > 50 and len(sentence) < max_length:
                          return sentence.strip()

                  # Fallback: use beginning of content
                  if len(clean_text) > max_length:
                      return clean_text[:max_length-3] + '...'

                  return clean_text or title

              def extract_keywords(self, title, content, tags, max_keywords=10):
                  """Extract SEO keywords"""
                  # Start with tags
                  keywords = set(tags[:5])

                  # Add important words from title
                  title_words = re.findall(r'\b[a-z]{4,}\b', title.lower())
                  important_title_words = [w for w in title_words if w not in 
                                          ['this', 'that', 'with', 'from', 'have', 'been']]
                  keywords.update(important_title_words[:3])

                  return list(keywords)[:max_keywords]

          # Load RSS sources function (keep existing)
          def load_rss_sources_by_category():
              """Load RSS URLs from category-specific files"""
              categories = {}
              source_files = glob.glob('rss-sources/*.txt')

              for file_path in source_files:
                  category = os.path.basename(file_path).replace('.txt', '')

                  with open(file_path, 'r') as f:
                      feeds = []
                      for line in f:
                          line = line.strip()
                          if line and not line.startswith('#'):
                              feeds.append(line)

                      if feeds:
                          categories[category] = feeds
                          print(f"üìÇ Loaded {len(feeds)} feeds for category: {category}")

              return categories

          # Keep existing helper functions
          def load_posted_urls():
              try:
                  if os.path.exists('_data/rss_tracking/posted_urls.json'):
                      with open('_data/rss_tracking/posted_urls.json', 'r') as f:
                          return json.load(f)
                  else:
                      os.makedirs('_data/rss_tracking', exist_ok=True)
                      return {"posted_urls": []}
              except Exception as e:
                  print(f"Error loading posted URLs: {e}")
                  return {"posted_urls": []}

          def save_posted_url(url):
              try:
                  posted_data = load_posted_urls()
                  posted_data["posted_urls"].append(url)
                  posted_data["posted_urls"] = posted_data["posted_urls"][-1000:]
                  with open('_data/rss_tracking/posted_urls.json', 'w', encoding='utf-8') as f:
                      json.dump(posted_data, f, indent=2)
              except Exception as e:
                  print(f"Error saving posted URL: {e}")

          def is_recent_article(published_date, category, max_age_hours=None):
              if max_age_hours is None:
                  category_rules = {
                      'extreme-weather': 24,
                      'environmental-news': 48,
                      'research-papers': 168,
                      'climate-science': 72,
                      'social-impact': 96,
                      'media': 72
                  }
                  max_age_hours = category_rules.get(category, 72)

              try:
                  if isinstance(published_date, str):
                      for fmt in ['%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%d %H:%M:%S']:
                          try:
                              article_date = datetime.strptime(published_date, fmt)
                              break
                          except ValueError:
                              continue
                      else:
                          return False
                  else:
                      article_date = datetime(*published_date[:6])

                  if article_date.tzinfo is None:
                      article_date = article_date.replace(tzinfo=timezone.utc)

                  cutoff_date = datetime.now(timezone.utc) - timedelta(hours=max_age_hours)
                  return article_date > cutoff_date
              except Exception as e:
                  print(f"Error checking article date: {e}")
                  return False

          def clean_title(title):
              # Escape quotes
              title = title.replace('"', '\\"')

              # Escape backslashes (for LaTeX)
              title = title.replace('\\', '\\\\')

              # Handle Liquid syntax
              title = title.replace('{{', '{ {')
              title = title.replace('}}', '} }')

              # Simplify LaTeX notation
              title = re.sub(r'\\mathbb{(\w)}', r'\1', title)  # \mathbb{R} ‚Üí R
              title = re.sub(r'\\mathrm{(\w+)}', r'\1', title) # \mathrm{m} ‚Üí m

              # Remove newlines
              title = title.replace('\n', ' ').strip()

              return title

          def clean_content(content):
              # Remove HTML tags
              content = re.sub(r'<[^>]+>', '', content)

              # Escape quotes
              content = content.replace('"', '\\"')

              # Escape backslashes
              content = content.replace('\\', '\\\\')

              # Handle Liquid syntax with raw tags
              content = re.sub(r'(\{\{.*?\}\})', r'{% raw %}\1{% endraw %}', content)

              # Clean whitespace
              content = re.sub(r'\s+', ' ', content)

              return content.strip()

          # Enhanced Jekyll post creation with SEO
          def create_jekyll_posts_from_feed(feed_url, category):
              try:
                  print(f"üîÑ Processing {category}: {feed_url}")
                  feed = feedparser.parse(feed_url)

                  if not feed or not hasattr(feed, 'entries'):
                      print(f"‚ùå Failed to parse feed: {feed_url}")
                      return 0

                  # Initialize tagger
                  tagger = SmartTagger()

                  posted_data = load_posted_urls()
                  posted_urls = posted_data["posted_urls"]
                  new_posts_count = 0

                  print(f"üì° Feed: {feed.feed.get('title', 'Unknown')} ({len(feed.entries)} entries)")

                  category_limits = {
                      'extreme-weather': 3,
                      'environmental-news': 2,
                      'research-papers': 1,
                      'climate-science': 2,
                      'social-impact': 2,
                      'media': 1
                  }
                  max_posts = category_limits.get(category, 2)

                  for entry in feed.entries[:15]:
                      try:
                          if entry.link in posted_urls:
                              continue

                          if hasattr(entry, 'published_parsed') and not is_recent_article(entry.published_parsed, category):
                              continue

                          # Prepare post data
                          title = clean_title(entry.title)

                          # Handle date formatting
                          try:
                              if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                  date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S") + " +0000"
                              else:
                                  date = datetime.now().strftime("%Y-%m-%d %H:%M:%S") + " +0000"
                          except:
                              date = datetime.now().strftime("%Y-%m-%d %H:%M:%S") + " +0000"

                          # Extract content
                          if hasattr(entry, 'content') and entry.content:
                              content = clean_content(entry.content[0].value)
                          elif hasattr(entry, 'summary'):
                              content = clean_content(entry.summary)
                          else:
                              content = "Read the full article at the link above."

                          # Extract smart tags
                          tags = tagger.extract_tags(title, content, category)

                          # Generate SEO description
                          seo_description = tagger.generate_seo_description(title, content)

                          # Extract keywords
                          keywords = tagger.extract_keywords(title, content, tags)

                          # Create unique filename
                          url_hash = hashlib.md5(entry.link.encode()).hexdigest()[:8]
                          slug = re.sub(r'[^a-z0-9]+', '-', title.lower())
                          filename = f"_posts/{date.split()[0]}-{category}-{slug[:30]}-{url_hash}.md"

                          if os.path.exists(filename):
                              continue

                          # Create Jekyll post with enhanced front matter
                          post_content = f"""---
          layout: feed_item
          title: "{title}"
          date: {date}
          categories: [{category}]
          tags: {tags}
          keywords: {keywords}
          description: "{seo_description}"
          external_url: {entry.link}
          is_feed: true
          source_feed: "{feed.feed.get('title', 'Unknown Source')}"
          feed_category: "{category}"
          ---

          {content}

          [Read original article]({entry.link})
          """

                          # Write post file
                          with open(filename, 'w', encoding='utf-8') as f:
                              f.write(post_content)

                          save_posted_url(entry.link)
                          new_posts_count += 1
                          print(f"‚úÖ Created: {title[:50]}... with {len(tags)} tags")

                          if new_posts_count >= max_posts:
                              break

                      except Exception as e:
                          print(f"‚ùå Error processing entry: {e}")

                  print(f"üìä Created {new_posts_count} new posts from {category}")
                  return new_posts_count

              except Exception as e:
                  print(f"‚ùå Error processing feed {feed_url}: {e}")
                  return 0

          # Main processing
          def main():
              print("üöÄ Starting multi-category RSS processing with smart SEO...")

              os.makedirs('_posts', exist_ok=True)
              os.makedirs('_data/rss_tracking', exist_ok=True)

              categories = load_rss_sources_by_category()

              if not categories:
                  print("‚ùå No RSS source files found")
                  return

              total_posts = 0

              for category, feeds in categories.items():
                  print(f"\nüìÇ Processing category: {category.upper()}")
                  print(f"üì° {len(feeds)} feeds to process")

                  category_posts = 0
                  for feed_url in feeds:
                      posts_created = create_jekyll_posts_from_feed(feed_url, category)
                      category_posts += posts_created
                      total_posts += posts_created

                  print(f"üìä Category {category}: {category_posts} total posts created")

              print(f"\n‚úÖ RSS processing complete! Total posts created: {total_posts}")

          if __name__ == "__main__":
              main()
          EOF

      - name: Commit processed posts
        run: |
          git config --global user.name 'Climate RSS Bot - Multi-Category with SEO'
          git config --global user.email 'action@github.com'
           git add -A
            if git diff --quiet --staged; then
              git commit -m "..." && git push
            fi
