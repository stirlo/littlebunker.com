name: Update RSS Feeds

on:
  schedule:
    - cron: '0 * * * *'  # Every hour
  workflow_dispatch:  # Manual trigger

jobs:
  update-feeds:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Create data directories
        run: |
          mkdir -p _data/feeds
          mkdir -p _data/metrics

      # Update Climate Metrics
      - name: Update Climate Metrics
        run: |
          cat > _data/metrics.yml << EOL
          co2:
            current: 421.5
            change: 2.5
          ch4:
            current: 1908
            change: 8.5
          temperature:
            overshoot: 1.2
          population:
            total: 8.1
            growth: 67
          EOL

      # Set up Python for feed processing
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Python dependencies
        run: pip install feedparser pyyaml requests

      # Fetch feeds and create posts with Python
      - name: Fetch Feeds and Create Posts
        run: |
          mkdir -p _data/feeds
          mkdir -p _posts/feeds/climate-science
          mkdir -p _posts/feeds/environmental-news

          python - << 'EOF'
          import feedparser
          import yaml
          import datetime
          import re
          import os
          import json
          import requests

          def clean_title(title):
              return title.replace('"', '\\"')

          def clean_content(content):
              # Remove HTML tags
              content = re.sub(r'<[^>]+>', '', content)
              # Replace quotes
              content = content.replace('"', '\\"')
              return content

          def fetch_feed(url):
              try:
                  feed = feedparser.parse(url)

                  # Save feed to JSON
                  feed_name = url.split('/')[-2] if url.endswith('/') else url.split('/')[-1]
                  feed_name = re.sub(r'[^a-z0-9]+', '-', feed_name.lower())

                  feed_data = {
                      "title": feed.feed.get('title', 'Unknown Feed'),
                      "link": feed.feed.get('link', url),
                      "description": feed.feed.get('description', ''),
                      "items": []
                  }

                  for entry in feed.entries[:10]:
                      item = {
                          "title": entry.get('title', 'No Title'),
                          "link": entry.get('link', ''),
                          "pubDate": entry.get('published', datetime.datetime.now().strftime("%a, %d %b %Y %H:%M:%S %z")),
                          "description": entry.get('summary', '')
                      }
                      feed_data["items"].append(item)

                  with open(f"_data/feeds/{feed_name}.json", 'w') as f:
                      json.dump(feed_data, f, indent=2)

                  return feed
              except Exception as e:
                  print(f"Error fetching {url}: {e}")
                  return None

          def create_posts_from_feed(feed_url, category, output_dir):
              feed = fetch_feed(feed_url)
              if not feed:
                  return

              for entry in feed.entries[:5]:  # Get top 5 entries
                  try:
                      title = clean_title(entry.title)

                      # Get date or use current date
                      try:
                          date = datetime.datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                      except:
                          date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                      # Get content
                      if 'content' in entry:
                          content = clean_content(entry.content[0].value)
                      elif 'summary' in entry:
                          content = clean_content(entry.summary)
                      else:
                          content = ""

                      # Create filename
                      slug = re.sub(r'[^a-z0-9]+', '-', title.lower())
                      filename = f"{output_dir}/{date.split()[0]}-{slug[:40]}.md"

                      # Create post content
                      post_content = f"""---
          layout: feed_item
          title: "{title}"
          date: {date}
          categories: [{category}]
          external_url: {entry.link}
          is_feed: true
          ---

          {content}
          """

                      # Write to file
                      with open(filename, 'w') as f:
                          f.write(post_content)
                  except Exception as e:
                      print(f"Error processing entry: {e}")

          # Climate Science feeds
          climate_science_feeds = [
              "https://climate.nasa.gov/feed/",
              "https://www.ipcc.ch/feed/",
              "https://www.climate.gov/news-features/feed"
          ]

          # Environmental News feeds
          environmental_news_feeds = [
              "https://insideclimatenews.org/feed/",
              "https://www.carbonbrief.org/feed/"
          ]

          # Process all feeds
          for feed_url in climate_science_feeds:
              create_posts_from_feed(feed_url, "climate-science", "_posts/feeds/climate-science")

          for feed_url in environmental_news_feeds:
              create_posts_from_feed(feed_url, "environmental-news", "_posts/feeds/environmental-news")
          EOF

      # Commit changes
      - name: Commit changes
        run: |
          git config --global user.name 'GitHub Action'
          git config --global user.email 'action@github.com'
          git add -A
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update climate feeds and metrics" && git push)
